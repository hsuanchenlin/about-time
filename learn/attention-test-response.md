# Attention in Transformer - Elaborated Answers (Test)

**Generated:** 2026-01-31 07:41 UTC

**Original Resource:** https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=7

---

## Your Current Understanding

- Embedding could represent meaning
- Attention(Q,K,V)
- masking beofre softmax is needed to avoid the sum is not 1.
- the size of the parameters is the square of the context size.
- calculate the value and use it as delta to improve the model

---

## Questions to Explore

### 1. low rank transformation. why the part of the matrix could be dropped?

*[AI elaboration will be generated by GitHub Action workflow]*

### 2. cross attention. is it important compared to self-attention? can't understand the difference

*[AI elaboration will be generated by GitHub Action workflow]*

### 3. multi-headed attention. dont'understand it

*[AI elaboration will be generated by GitHub Action workflow]*

### 4. output matrix. no idea either

*[AI elaboration will be generated by GitHub Action workflow]*

