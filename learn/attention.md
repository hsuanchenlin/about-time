Attention in Transformer
---
resource: 
https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=7
---

# Known
- Embedding could represent meaning

- Attention(Q,K,V)
Q: input query
K: key, word in the model
V: value for the weight

- masking beofre softmax is needed to avoid the sum is not 1.

- the size of the parameters is the square of the context size.

- calculate the value and use it as delta to improve the model

# Known Unknown

- low rank transformation. why the part of the matrix could be dropped?
- cross attention. is it important compared to self-attention? can't understand the difference
- multi-headed attention. dont'understand it
- output matrix. no idea either
